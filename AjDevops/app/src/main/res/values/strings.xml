<resources>
    <string name="app_name">AjDevops</string>
    <string name="title_activity_navigation_drawer">NavigationDrawerActivity</string>
    <string name="navigation_drawer_open">Open navigation drawer</string>
    <string name="navigation_drawer_close">Close navigation drawer</string>
    <string name="nav_header_title">Android Studio</string>
    <string name="nav_header_subtitle">android.studio@android.com</string>
    <string name="nav_header_desc">Navigation header</string>
    <string name="action_settings">Settings</string>
    <!--feedback-->
    <string name="mail_feedback_email">feedback@example.com</string>
    <string name="mail_feedback_subject">Feedback on your app</string>
    <string name="mail_feedback_message">Hi, \n\nYour Feedback sample app rocks! I would like to give you some feedback:</string>
    <string name="title_send_feedback">Send feedback</string>
    <string name="sendPhoto">Photo Camera</string>
    <string name="sign_out">Sign Out</string>
    <string name="select_picture_title">Get Photo From</string>
    <string name="sendPhotoGallery">Photo Gallery</string>
    <string name="api_key_google_places">AIzaSyBisbsq9wIW8Q2Isj033c3wXqtTBRyaRBA</string>
    <string name="sendLocation">Send Location</string>
    <string name="locationTo">Localização Enviada</string>

    <string name="add_task_button">Add Task</string>

    <string name="banner_ad_unit_id">ca-app-pub-9279514970367399/1797145096</string>
    <!-- TODO: Remove or change this placeholder text -->
    <string name="hello_blank_fragment">Hello blank fragment</string>
    <string name="history">Git is, first and foremost, a version control system (VCS)and it is open source.
    \n \n <font color='#800080'><b>There are two types of (VCS)</b>\n.</font> \n 1. Distributed version control system (DVCS)\n2.Centralised version control systems (CVCS).\n\n There are many version control systems out
    there, for example, CVS, SVN, Mercurial, Fossil etc. \n Git is an example of Distributed version
    control System (DVCS), \n which keeps track of each modification done to the code over time, and
    allows you to backtrack if necessary and undo those changes.\n\n Git will allow you to go back
    to a previous status on a project or to see its entire evolution since the project created.\n\n
    <b>Git is also called as source code management (SCM).</b>

    <font fgcolor='#800080'>
      <b>\n\n With Git, 3 basic issues were solved when working on projects:</b>\n
    </font>
    \n\n

    1===>It has made easier to manage large projects.\n
    2===>It helps you to avoid overwriting the team’s advances and work.\n
    3===>With git, you just pull the entire code and history to your system. It’s much simpler and much
    more lightweight.\n
    4===><b>Files in a repository go through three stages before being under version control with git:\n\n</b>

    <font color='#800080'>
      <b>Working Directory (Untracked):</b>\n
    </font>
    \n All the modifications are done in this stage to the files, but is not part of git’s version
    control.\n So, to make files part of git version control we use the below command \n<b>#git add
      or #git add . (dot means everything).\n
    </b> \n
    <font color='#800080'>
      <b>Staging (Staged):</b>\n
    </font>
    \n All the files have been added to git’s version control and are
    tracked by git, but changes have not been committed,\n so to commit changes we use following
    command \n <b>#git commit -m “commit message”.\n</b> \n
    <font color='#800080'>
      <b>Committed:</b>\n
    </font>
    \n All the changes has been committed.\n
    There are many tools available in the market right now like Git to revision control and SCM
    (source code management) but \n\n
    <font color='#800080'><b>why Git is the most popular? Well the reasons are:</b>\n\n
    </font>

    1===>Git tracks state, history and integrates of the source tree.\n
    2===>Git keeps old versions for you if any developer occurs any mistake in code, \n then you will
    always have the previous version to fix it.\n
    3===>Multiple developers can work together, once they write code in their local machine and commit it
    \n then other developers can pull it easily.\n
    4===>Large developers community and online websites to upload \n your source codes or get others
    source codes to make your work easier.\n
    5===>Lots of software available for both who comfortable with command line and for others GUI
    tools.\n
    6===>Easy and clear documentation to get started with.\n
    7===>Git will not use much bandwidth you don’t have to connect with your server always\n you just
    need to connect to push code when you complete the code.\n
    <font color='#800080'>
      <b>Git uses some repositories management services like</b>
    </font>
    \n\n Github, Gitlab, Bitbucket
  </string>

    //docker details
    <string name="docker_history">Linux containers are an operating system level virtualization technology for providing multiple isolated Linux environments on a single Linux host. Unlike virtual machines (VMs), containers do not run dedicated guest operating systems. Rather, they share the host operating system kernel and make use of the guest operating system system libraries for providing the required OS capabilities. Since there is no dedicated operating system, containers start much faster than VMs.\n\n
    <font color='#800080'><b>Virtual Machines Vs ContainersImage credit: Docker Inc.</b></font>\n\n
Containers make use of Linux kernel features such as Namespaces, Apparmor, SELinux profiles, chroot, and CGroups for providing an isolated environment similar to VMs. Linux security modules guarantee that access to the host machine and the kernel from the containers is properly managed to avoid any intrusion activities.\n In addition containers can run different Linux distributions from its host operating system if both operating systems can run on the same CPU architecture.\n
In general, containers provide a means of creating container images based on various Linux distributions, an API for managing the lifecycle of the containers, client tools for interacting with the API, features to take snapshots, migrating container instances from one container host to another, etc.\n
  <font color='#800080'>
<b>Container History</b>
    </font>\n
Below is a short summary of container history extracted from Wikipedia and other sources:\n
<font color='#800080'><b>1979 — chroot</b></font>\n
The concept of containers was started way back in 1979 with UNIX chroot.
    It’s an UNIX operating-system system call for changing the root directory of a
    process and it is children to a new location in the filesystem which is only visible
    to a given process.The idea of this feature is to provide an isolated disk space for each
    process. Later in 1982 this was added to BSD.\n
    <font color='#800080'><b>2000 — FreeBSD Jails</b></font>\n
FreeBSD Jails is one of the early container technologies introduced by Derrick T. Woolworth at R and D Associates for FreeBSD in year 2000. It is an operating-system system call similar to chroot, but included additional process sandboxing features for isolating the filesystem, users, networking, etc. As a result it could provide means of assigning an IP address for each jail, custom software installations and configurations, etc.\n
<font color='#800080'><b>2001 — Linux VServer</b></font>\n
Linux VServer is a another jail mechanism that can be used to securely partition resources on a computer system (file system, CPU time, network addresses and memory). Each partition is called a security context, and the virtualized system within it is called a virtual private server.\n
<font color='#800080'><b>2004 — Solaris Containers</b></font>\n
Solaris Containers were introduced for x86 and SPARC systems, first released publicly in February 2004 in build 51 beta of Solaris 10, and subsequently in the first full release of Solaris 10, 2005. A Solaris Container is a combination of system resource controls and the boundary separation provided by zones. Zones act as completely isolated virtual servers within a single operating system instance.\n
<font color='#800080'><b>2005 — OpenVZ</b></font>\n
OpenVZ is similar to Solaris Containers and makes use of a patched Linux kernel for providing virtualization, isolation, resource management, and checkpointing. Each OpenVZ container would have an isolated file system, users and user groups, a process tree, network, devices, and IPC objects.\n
<font color='#800080'><b>2006 — Process Containers</b></font>\n
Process Containers was implemented at Google in year 2006 for limiting, accounting, and isolating resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. Later on it was renamed to Control Groups to avoid the confusion multiple meanings of the term “container” in the Linux kernel context and merged to the Linux kernel 2.6.24. This shows how early Google was involved in container technology and how they have contributed back.\n
<font color='#800080'><b>2007 — Control Groups</b></font>\n
As explained above, Control Groups AKA cgroups was implemented by Google and added to the Linux Kernel in 2007.\n
<font color='#800080'><b>2008 — LXC</b></font>\n
LXC stands for LinuX Containers and it is the first, most complete implementation of Linux container manager. It was implemented using cgroups and Linux namespaces. LXC was delivered in liblxc library and provided language bindings for the API in Python3, Python2, Lua, Go, Ruby, and Haskell. Contrast to other container technologies LXC works on vanila Linux kernel without requiring any patches. Today LXC project is sponsored by Canonical Ltd. and hosted here.\n
<font color='#800080'><b>2011 — Warden</b></font>\n
Warden was implemented by CloudFoundry in year 2011 by using LXC at the initial stage and later on replaced with their own implementation. Unlike LXC, Warden is not tightly coupled to Linux. Rather, it can work on any operating system that can provide ways of isolating environments. It runs as a daemon and provides an API for managing the containers. Refer to Warden documentation and this blog post for more detailed information on Warden.\n
<font color='#800080'><b>2013 — LMCTFY</b></font>\n
lmctfy stands for “Let Me Contain That For You”. It is the open source version of Google’s container stack, which provides Linux application containers. Google started this project with the intention of providing guaranteed performance, high resource utilization, shared resources, over-commitment, and near zero overhead with containers (Ref: lmctfy presentation). The cAdvisor tool used by Kubernetes today was started as a result of lmctfy project. The initial release of lmctfy was made in Oct 2013 and in year 2015 Google has decided to contribute core lmctfy concepts and abstractions to libcontainer. As a result now no active development is done in LMCTFY.\n
The libcontainer project was initially started by Docker and now it has been moved to Open Container Foundation.\n
<font color='#800080'><b>2013 — Docker</b></font>\n
Docker is the most popular and widely used container management system as of January 2016. It was developed as an internal project at a platform-as-a-service company called dotCloud and later renamed to Docker. Similar to Warden, Docker also used LXC at the initial stages and later replaced LXC with it’s own library called libcontainer. Unlike any other container platform, Docker introduced an entire ecosystem for managing containers. This includes a highly efficient, layered container image model, a global and local container registries, a clean REST API, a CLI, etc. At a later stage, Docker also took an initiative to implement a container cluster management solution called Docker Swarm.\n
<font color='#800080'><b>2014 — Rocket</b></font>\n
Rocket is a much similar initiative to Docker started by CoreOS for fixing some of the drawbacks they found in Docker. CoreOS has mentioned that their aim is to provide more rigorous security and production requirements than Docker. More importantly, it is implemented on App Container specifications to be a more open standard. In addition to Rocket, CoreOS also develops several other container related products used by Docker and Kubernetes: CoreOS Operating System, etcd, and flannel.\n
<font color='#800080'><b>2016 — Windows Containers</b></font>\n
Microsoft also took an initiative to add container support to the Microsoft Windows Server operating system in 2015 for Windows based applications, called Windows Containers. This is to be released with Microsoft Windows Server 2016. With this implementation Docker would be able to run Docker containers on Windows natively without having to run a virtual machine to run Docker (earlier Docker ran on Windows using a Linux VM).\n
<font color='#800080'><b>The Future of Containers</b></font>\n
As of today (Jan 2016) there is a significant trend in the industry to move towards containers from VMs for deploying software applications. \n\n The main reasons for this are the flexibility and low cost that containers provide compared to VMs. Google has used container technology for many years with Borg and Omega container cluster management platforms for running Google applications at scale. More importantly, Google has contributed to container space by implementing cgroups and participating in libcontainer projects. Google may have gained a huge gain in performance, resource utilization, and overall efficiency using containers during past years. Very recently Microsoft, who did not had an operating system level virtualization on the Windows platform took immediate action to implement native support for containers on Windows Server.\n
Docker, Rocket, and other container platforms cannot run on a single host in a production environment, the reason is that they are exposed to single point of failure. \n While a collection of containers are run on a single host, if the host fails, all the containers that run on that host will also fail.\n  To avoid this, a container host cluster needs to be used. One of the first most open source container cluster management platforms to solve this problem was Apache Mesos.\n It was initially developed at University of California, Berkeley as a research project and later moved to Apache in around year 2012. Google took a similar step to implement a cutting edge, open source container cluster management system called Kubernetes in year 2014 with the experience they got from Borg. \n Docker also started a solution called Docker Swarm in year 2015.\n  Today these solutions are at their very early stages and it may take several months and may be another year to complete their full feature set, become stable and widely used in the industry in production environments.\n
Microservices are another groundbreaking technology rather a software architecture which uses containers for their deployment.\n A microservice is nothing new but a lightweight implementation of a web service which can start extremely fast compared to a standard web service.\n This is done by packaging a unit of functionality (may be a single service/API method) in one service and embedding it into a lightweight web server binary.\n
By considering the above facts we can predict that in next few years,
    containers may take over virtual machines, and sometimes might
    replace them completely.
</string>

    //jenkins details

    <string name="jenkins_history">Jenkins is a Continuous Integration (CI) server or tool which is written in java. \n It provides Continuous Integration services for software development,\n <b>which can be started via command line or web application server</b>. \n And also,it is happy to know that Jenkins is free software to download and install.\n \n
  <font color='#800080'><b>Before going in details to Jenkins, let me tell you what Continuous Integration (CI) is.</b></font>\n\n
1====>Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day.\n 2===> It is a process of running your tests on a non-developer (say testers) machine automatically when someone pushes new code into the source repository.\n
 <font color='#800080'><b>  Some of the attractive reasons why you need automate build testing and integration are:</b></font>\n\n
<font color='#800080'><b>1.Developer time is concentrated on work that matters:</b></font>\n  Most of the work like integration and testing is managed by automated build and testing systems. So the developer’s time is saved without wasting on large-scale error-ridden integrations.\n\n
<font color='#800080'><b>2.Software quality is made better: </b></font>\nIssues are detected and resolved almost right away which keeps the software in a state where it can be released at any time safely.\n\n
<font color='#800080'><b>3.Makes development faster:</b></font>\n Most of the integration work is automated. Hence integration issues are less. This saves both time and money over the lifespan of a project.\n
Continuous Build System can include tools like Jenkins, Bamboo, and Cruise Control, etc. Bamboo has better UX support but it is not a free tool. Jenkins is an open source tool, easier to setup and configure and also has a very active plug-in development community which makes it favored. Now, let us dive into the Jenkins tool.\n\n
 <font color='#800080'><b> Jenkins History</b></font>\n\n
Jenkins was originally developed as the Hudson project. Hudson’s creation started in summer of 2004 at Sun Microsystems. It was first released in java.net in Feb. 2005.\n
<b>During November 2010</b>,\n an issue arose in the Hudson community with respect to the infrastructure used, which grew to encompass questions over the stewardship and control by Oracle. Negotiations between the principal project contributors and Oracle took place, and although there were many areas of the agreement a key sticking point was the trademarked name “Hudson” after Oracle claimed the right to the name and applied for a trademark in December 2010. As a result, on January 11, 2011, a call for votes was made to change the project name from “Hudson” to “Jenkins”. The proposal was overwhelmingly approved by community vote on January 29, 2011, creating the Jenkins project.\n
<b>On February 1, 2011,</b>\n Oracle said that they intended to continue development of Hudson, and considered Jenkins a fork rather than a rename. Jenkins and Hudson, therefore, continue as two independent projects, each claiming the other is the fork.\n<b>As of December 2013,\n</b>  the Jenkins organization on GitHub had 567 project members and around 1,100 public repositories, compared with Hudson’s 32 project members and 17 public repositories.\n\n
    <font color='#800080'><b>Let us depict a scenario where the complete source code of the application was built and then deployed on the test server for testing. It sounds like a robust way to develop software, but this method has many weaknesses. They are,</b></font>\n\n
1===>Developers have to pause till the complete software is developed for the test results.\n
2===>There is a huge possibility that the test results might show lot many bugs. This makes developers be in a complex situation to find the root cause of those bugs since they have to check the entire source code of the application.
Delivery process of software is slowed down.\n
3===>Continuous feedback referring to things like coding or architectural issues, build failures, test condition and file release uploads were missing so that the quality of software can go down.\n
4===>The whole process was manual which increments the risk of repeated failure.\n
5===>It is obvious from the above-stated problems that along with slow software delivery process, the quality of software also went down.\n This leads to customer unhappiness.\n So, to overcome such confusion there was a crucial demand for a system to exist where developers can gradually trigger a build and test for each and every change made in the source code. \n Therefore, Jenkins tool is used in CI. It is the most mature CI tool possible. Now let us see how Continuous Integration with Jenkins crushes the above shortcomings.\n
For software development, we can hook it up with most of the repositories like SVN, Git, Mercurial, etc. \n Jenkins has lots of plugins that are available freely. These plugins help to integrate with various software tools for better convenience.\n
One really nice thing about Jenkins is, build configuration files will be on disk which makes massive build cloning and reconfiguring easy.\n\n
    <font color='#800080'><b> Advantages of Jenkins</b></font>\n\n
1.Jenkins is an open source tool with much support from its community.\n
2.Installation is easier.\n
3.It has more than 1000 plug-in to make the work easier.\n
4.It is easy to create new Jenkins plugin if one is not available.\n
5.It is a tool which is written in Java. Hence it can be portable to almost all major platforms.\n\n</string>

    //ansible details
    <string name="ansible_history">Ansible is an open source tool used to deploy applications to remote nodes and provision servers in a repeatable way.\n It gives you a common framework for pushing multi-tier applications and application artifacts using a push model setup, although you can set it up as master-client if you’d like.\n Ansible is built on playbooks that you can apply to an extensive variety of systems for deploying your app.\n\n
        <font color='#800080'> <b> When to use it:</b> </font>\n If getting up and running quickly and easily is important to you and you don’t want to install agents on remote nodes or managed servers, consider Ansible. \n It’s good if your need or focus is more on the system administrator side. Ansible is focused
        on being streamlined and fast, so if those are key concerns for you, give it a shot.\n

 <font color='#800080'> <b>Price:</b> </font>\n Free open source version, with paid plans for Ansible Tower starting at $5,000 per year (which gives you up to 100 nodes).\n\n
 <font color='#800080'> <b>Pros:</b> </font>\n\n
1===>SSH-based, so it doesn’t require installing any agents on remote nodes.\n
2===>Easy learning curve thanks to the use of YAML.\n
3===>Playbook structure is simple and clearly structured.\n
4===>Has a variable registration feature that enables tasks to register variables for later tasks\n
5===>Much more streamlined code base than some other tools\n\n
  <font color='#800080'> <b>Cons:</b> </font>\n\n
1===>Less powerful than tools based in other programming languages.\n
2===>Does its logic through its DSL, which means checking in on the documentation frequently until you learn it\n
3===>Variable registration is required for even basic functionality, which can make easier tasks more complicated\n
4===>Introspection is poor. Difficult to see the values of variables within the playbooks\n
5===>No consistency between formats of input, output, and config files\n
6===>Struggles with performance speed at times.\n\n

<font color='#800080'> <b>Basic Structure of an Ansible Project</b> </font>\n\n

As part of the deployment script, we must define the Ansible structure first. In the structure, we need to define staging, production, group_vars, host_vars and roles:


1===>The staging file will keep all test environments information\n
2===>The production file will keep all production and Disaster Recovery (DR) environments information\n
3===>All application server details should be mentioned in host_vars\n
4===>All variables used in Ansible need to be furnished in group_vars\n
5===>All templates, files and tasks are defined in roles\n

Finally, we need to define a wrapper Ansible playbook to execute the roles. Under one wrapper playbook, the number of roles that will execute depends on developer needs or the project requirements. In the following subsections, I will explain each and every component of an Ansible project.\n\n
<font color='#800080'><b>Inventory</b></font>\n

Inventory plays an important role in identifying which server we will need to deploy the application using Ansible. As a best practice, it is always good to use a separate inventory for pre-production (i.e. test environments and production environment application server details). As a standard, we use it to maintain Staging and Production Ansible inventory for test and production servers respectively.\n

<font color='#800080'><b>Group Variables (group_vars)</b></font>\n

Group variables contain all environmental variables as well as common variables. This is the place where we can store all template variables for each environment. While running an Ansible playbook, we will specify a limit, and based on that, Ansible will use the appropriate group variables.\n


<font color='#800080'><b>Host Variables (host_vars)</b></font>\n

All the application servers’ IP addresses are stored in host_vars. In the runtime based on the inventory and limit environments, Ansible will identify the application server details from host_vars.\n


<font color='#800080'><b>Vault</b></font>\n

    Vault is a password-protected file where deployment engineers store all clear text passwords. Vault has the capability to use its own encryption to protect our passwords. These passwords might include a deployment user password, service account password, database password, and a web service password. We can use ansible-vault create or edit for creating and modifying a vault respectively.\n


<font color='#800080'><b>Roles</b></font>\n

The application deployment process always follows a set of sequential instructions. In Ansible, each instruction has been defined under a role by the software engineers. Each role consists of three components: tasks, templates and files.\n


<font color='#800080'><b>Tasks</b></font>\n

Tasks mainly perform a set of operations, which completely align with the roles objective. In order to perform the tasks, external files are kept under the files directory, and templates are kept under the templates directory. All operations under a particular task are furnished in the main.yml file.\n
<font color='#800080'><b>Templates</b></font>\n
For each application, all the environments have some common files. Some attributes of those common files are different based on their environment. We use templates to handle different environments with minimal changes. For example, say we have a database connection string defined in a file and that file must be deployed in all environments, but the database name in each environment is different. In that situation, the deployment engineer would create a template and keep the database name as a variable, and that variable defined under each environment group_vars along with their proper database name.\n
<font color='#800080'><b>Files</b></font>\n
Any type of file used by a task is kept under that task role directory. It may be any executable like .sh, .exe, .dat or a simple .txt file. An example of the basic structure of an Ansible project.
<font color='#800080'><b>Establishing Connectivity with Various Servers</b></font>\n
One of the most important prerequisites of an Ansible deployment is connecting it with other systems like application servers and version control systems (SVN, Bitbucket, Serena Dimension, Visual Source Safe, etc.)\n
        <font color='#800080'><b>Connectivity with various servers</b></font>\n
We need to establish connectivity between the version control server and the Ansible server to get the application components. We also need to deploy those source components into an application server by building connectivity between the Ansible server and the application server. In general, SSH is used for Red Hat Enterprise Linux (RHEL) system connectivity and NTLM or Kerberos is used for Windows systems.\n
        <font color='#800080'><b>Mechanism to Run an Ansible Playbook</b></font>\n
There are many parameters we need to consider while running an Ansible playbook. Consider a case where the developers have developed all the Ansible roles and also created some wrapper .yml files, which contain a set of roles to be executed as part of a particular deployment. The deployment engineer will run the wrapper as per their objective (deploying applications, properties, keystores, etc.)\n


 </string>

    <!--maven details-->
    <string name="maven_history">
Apache Maven is project management tool which is following the concept of a project object model (POM). Mavan can manage project’s build and documentation from a central place.\n <font color='#800080'><b>Maven 1  </b></font>\n
actually was started as a sub project of Apache Turbine in 2002 (by Sonatype’s Jason van Zyl).
    It was released in July 2004 as v1.0.\n <font color='#800080'><b>Maven 2  </b></font>\nwas released in Oct 2005.
    It was a complete rewrite of the previous project. It was not backward compatible.\n<font color='#800080'><b>Maven 3
</b></font>\nwas released in October 2010. It is same as Maven 2 but more stable.
    This article explores the very basic concepts needed for the beginners who want to
    learn and get started with the Maven. Before the popularity of using Maven, most of the projects
    used Ant Script for the projects build which is used only for the build purpose.\n\n
<b>Maven is a build and dependencies management system used primarily for Java projects.</b>\n\n
<font color='#800080'><b>Key features include: </b></font>\n\n
1===>Create new projects through archetypes.\n
2===>Project configuration in POM file and Settings file\n
3===>Project building using lifecycles, phases, plugins, goals and build profiles.\n
4===>Dependency management through repositories\n
5===>Deployment with the release plugin.\n\n
 <font color='#800080'><b>1.The Project Object Model </b></font>\n\n
The Project Object Model or POM is the fundamental unit of work in Maven. \nIt is an XML file, usually defined in
the project root directory, that contains information about the project and the configuration used by
Maven to build the project.\n\n
<font color='#800080'><b>The configuration that can be included in the POM file is as follows: </b></font>\n\n
1===>Plugins and goals\n
2===>Dependencies\n
3===>Repositories\n
4===>Build profiles\n
5===>Project metadata such as version, description, developers, etc.\n
To facilitate a default configuration for all project, Maven provides what is known as the Super POM.\n
The Super POM is Maven default POM. All POMs extend the Super Pom thus inheriting the configuration specified in the Super POM.\n\n
<font color='#800080'><b>The Settings File </b></font>\n\n
As mentioned above, the POM file contains the project configuration, whilst the Settings file contains
the user specific configuration.\n There can be two settings files, the Global settings file, situated in the Maven
install directory, and the user settings file that is situated in the user home directory.\n\n
<font color='#800080'><b>The settings file can provide the following configuration: </b></font>\n\n
1===>Simple values\n
2===>Plugin groups\n
3===>Server credentials\n
4===>Proxies\n
5===>Profiles\n\n
<font color='#800080'><b>Build Lifecycles </b></font>\n\n
Maven build process is based on lifecycles. The lifecycle provides a clearly defined process for
building and distributing project artifacts.\n\n
<b>There are three different lifecycles in Maven.</b>\n\n
<font color='#800080'><b>Default: </b></font> Handles project building and deployment.\n
<font color='#800080'><b>Clean:  </b></font>Handles project cleaning.\n
<font color='#800080'><b>Site: </b></font> Handles project site docs.\n\n
<font color='#800080'><b>Phases </b></font>\n\n
Each lifecycle is defined by a series of stages named build phases.\nA build phase is responsible for a specific step in the lifecycle, but the way it carries out its duty depend on the plugin goals bound to the phase.\n\n
<font color='#800080'><b>Plugins </b></font>\n\n
Plugins are artifacts that provide goals for the build phases.\n Dividing the phases into goals, provided by plugins, make the build process really flexible and customizable.\n
A plugin can provide one or more goals. Each goal represents a capability of that plugin.\n
For example,\n Maven only supports a single source and test directories for a project.\nIf we decided to add additional directories to the project,we could use a plugin that provides goals to add source and test directories to the build process.\n\n
<font color='#800080'><b>Goals </b></font>\n\n
Goals are responsible for executing specific tasks during each phase.\nSome phases have default goals. For the default lifecycle, default goals are provided by the packaging option, defined in the project POM file.\nIn addition to the default goals, extra goals can be defined by configuring plugins in the project POM file.\n Therefore, a particular build phase can be composed of multiple goals.\n If a phase does not have any goals,it will not be executed as part of the lifecycle.\n\n
<font color='#800080'><b>Standalone Plugins</b></font>\n\n
Most of the plugins provide goals that are bound to build phases. However, there are some plugins that provide goals which are meant to be executed separately, not as part of the build lifecycle.\n\n
<font color='#800080'><b>The Archetype Plugin</b></font>\n\n
If you happen to be an IntelliJ user, you may have seen that when creating a new Maven project the first option is a list of archetypes. Have you ever wonder what is it for? I have to say that I did not until I found what archetypes are by digging into Maven features.\n
An archetype is simply an existing project template.\nThe Archetype plugin provides Maven project templates. It creates the project structure and POM file based on standard templates. The process of creating a new project is done in an interactive way by just providing project specific configuration such as groupid, artifact name, etc.\n
It helps to apply project or organisations best practices. New users can have, in seconds, a working project to use as a walking skeleton.\n
The plugin has additive support, meaning that can be used to add pieces to existing projects, i.e.\nMaven site archetype can quickly create a documentation site for the project.\n
Users can create their own archertypes in their organisations repository and use them as a base for new projects.\n
Being a standalone plugin, the archetype plugin provides goals that are not bound to any lifecycle.
    \nThe goals are executed directly, as opposed to what is done when using the lifecycle,
    where goals are executed as part of the lifecycle phases.\n\n
<font color='#800080'><b>Release Plugin</b></font>\n\n
Provide a standard mechanism to release project artifacts.\n\n
<font color='#800080'><b>The Release plugin has two main goals.</b></font>\n\n
<font color='#800080'><b>Prepare</b></font>\n\n
1.Verify there are not uncommitted changes.\n
2.Prompt user to provide a tag, release, and development version names.\n
3.Modify and commit release info in the POM file.\n
4.Tag entire project.\n\n
<font color='#800080'><b>Perform</b></font>\n\n
1.Extract file revisions under new tag name\n
2.Execute the Maven lifecycle on the extracted project instance\n
3.Deploy the artifacts to local and remote repositories\n\n
<font color='#800080'><b>Repositories</b></font>\n\n
<b>Maven uses repositories to hold build artifacts and dependencies.</b>\n\n
Maven Repositories are used as in Git, but storing build and dependencies artifacts instead of source code. Doing so, users can easily consume your project artifacts from the repositories.\n\n
There are two types of repositories local and remote, both are structured the same way.\n\n
1.The local repositories live in the users local machines and are used as a cache of the remote repositories, providing offline building capabilities.\n2.The remote repository can be divided into two subgroups, public and internal. Public repositories hold artifacts that are publicly available, whilst internal repositories are created in organisations to share internal artifacts between development teams.\n
It is strongly recommended that when using maven, dependencies JARs are not stored in source control, but in the repositories. Doing so, Maven is able to handle transitive dependencies, as all dependencies information is available through the POM file and the Maven repositories.\n\n
<font color='#800080'><b>Build Profiles</b></font>\n\n
Maven Build Profiles are used to facilitate portable builds.\nThe build profiles modify the POM file at build time to provide equivalent-but-difference parameters that are environment dependent.\nFor example, it is the perfect place to define filesystem references that are different for each user.\n
Profiles provide properties that can be referenced in the POM file. The properties are defined in the [properties] section in the profile declaration.\n
Build profiles can be declared in the POM file, as a per project definition, or in the Settings files. Build profiles defined in the Global Settings file are available for all users of the machine, whilst the ones defined in the User Settings file are available only for a particular user.\n\n
<font color='#800080'><b>Profiles are triggered in different ways:</b></font>\n\n
<font color='#800080'><b>1.Explicitly:</b></font> Running a Maven build through the command line, including -P option.\n
<font color='#800080'><b>2.Maven settings:</b></font> Including the profile in the [active profiles] section. When using this option the profile is always active.\n
<font color='#800080'><b>3.[activation]</b></font> section in profile declaration:
    The activation section can activate a certain profile based on environment variables,
    OS settings, and missing or present files.\n\n
<font color='#800080'><b>Conclusion</b></font>\n\n
<b>There are many areas where Maven can ease development effort:</b>\n\n
1.Easy build process\n
2.Uniform build system\n
3.Rich project information\n
4.Guidelines for best development\n
5.Transparent migration to new features\n
</string>

    <!--linux details-->
    <string name="linux_history">Linux is, in simplest terms, an Operating System. It is the software on a computer that enables applications and the computer operator to access the devices on the computer to perform desired functions. The Operating System (OS) relays instructions from an application to, for instance, the computer processor. The processor performs the instructed task, then sends the results back to the application via the Operating System.\n\nExplained in these terms, Linux is very similar to other Operating Systems, such as Windows and OS X.\n\n
 <font color='#800080'><b>What is Linux?</b></font>\n\n
 Linux is an example of Open Source software development and Free Operating System (OS).\n
  <font color='#800080'><b>Architectures </b></font>\n\n
Linux Originally developed for Intel is x86 hardware, ports available for over two dozen CPU types including ARM.\n
 <font color='#800080'><b>File system Support </b></font>\n\n
Linux use Ext2, Ext3, Ext4, Jfs, ReiserFS, Xfs, Btrfs format.\n
 <font color='#800080'><b>Usage </b></font>\n\n
Linux can be installed on a wide variety of computer hardware, ranging from mobile phones, tablet computers and video game consoles, to mainframes and supercomputers.\n
 <font color='#800080'><b>GUI </b></font>\n\n
Linux typically provides two GUIs, KDE and Gnome. But Linux GUI is optional.\n
 <font color='#800080'><b> Market Share for Desktop PC</b></font>\n\n
The market share of Linux is about 1.29%.\n
 <font color='#800080'><b>Threat Detection and Solution </b></font>\n\n
In case of Linux, threat detection and solution is very fast, as Linux is mainly community driven and whenever any Linux user post s any kind of threat, several developers start working on it from different parts of the world.\n
 <font color='#800080'><b>Cost </b></font>\n\n
Linux can be freely distributed, downloaded freely, distributed through magazines, Books etc. There are priced versions for Linux also, but they are normally cheaper than Windows.\n
 <font color='#800080'><b>Security </b></font>\n\n
Linux has had about 60-100 viruses listed till date.\n
 <font color='#800080'><b>Text Mode Interface </b></font>\n\n
BASH (Bourne Again Shell) is the Linux default shell. It can support multiple command interpreters.\n
 <font color='#800080'><b>Development and Distribution </b></font>\n\n
Linux is developed by Open Source development i.e. through sharing and collaboration of code and features through forums etc. and it is distributed by various vendors such as Debian, Red Hat, SUSE, Ubuntu, and GentuX etc.\n
 <font color='#800080'><b>User </b></font>\n\n
Linux Operating System for everyone, from home users to developers and computer enthusiasts alike.\n
 <font color='#800080'><b>Kernel </b></font>\n\n
Linux kernel is freely available. Linux kernel is developed by the community. Linus Torvalds oversees things.\n
 <font color='#800080'><b>Patches </b></font>\n\n
Linux patches are not highly tested as UNIX patches.\n
 <font color='#800080'><b> Linux Distribution Names</b></font>\n\n
<b>A few popular names:</b>\n
Redhat Enterprise Linux, Fedora Linux, Debian Linux, Suse Enterprise Linux, Ubuntu Linux\n
 <font color='#800080'><b> Linux Advantages</b></font>\n\n

One of the most valued advantages of Linux over the other platforms lies with the high security levels it ensures. Every Linux user is happy to work in a virus-free environment and use the regular virus-prevention time needed when working with other Operating Systems for other more important tasks.\n
Thanks to its open-source distribution, >Linux is being constantly developed and updated by the constantly expanding community of programmers supporting it. Despite its dynamic nature, it is totally complete in terms of functionality and interface. All those ongoing development efforts are made with the sole purpose of keeping the platform flexible and ever adaptable to the changeable climate of the WWW.\n
<font color='#800080'><b>Linux in World Wide We </b></font>\n\n
Due to its innate stability, the Linux-based distributions are a top choice for Internet servers, with a great part of the World Wide Web being powered by Linux. Linux is often used with Apache, thus creating the stable Linux-Apache combination.\n
As a fundamental part of the web, Linux has deservedly found its place in the popular LAMP open source web platform, which represents a combination between the most popular website building technologies: Linux, Apache (web server), MySQL (database) and PHP/Perl/Python (web application languages).\n
 <font color='#800080'><b>Disadvantages of Linux </b></font>\n\n
 <font color='#800080'><b>Understanding </b></font>\n
Becoming familiar with the Linux Operating System requires patience as well as a strong learning curve. You must have the desire to read and figure things out on your own, rather than having everything done for you.\n
 <font color='#800080'><b>Compatibility </b></font>\n
Because of its free nature, Linux is sometimes behind the curve when it comes to brand new hardware compatibility. Though the kernel contributors and maintainers work hard at keeping the kernel up to date, Linux does not have as much of a corporate backing as alternative Operating Systems. Sometimes you can find third party applications, sometimes you can not.</string>
</resources>
